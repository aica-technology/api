"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[1772],{635:(e,n,t)=>{t.d(n,{A:()=>o});const o=t.p+"assets/images/object-detection-using-yolo-graph-fce0eefbec30aec20cc0ab84930b2da8.png"},2234:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>c,default:()=>p,frontMatter:()=>s,metadata:()=>o,toc:()=>d});const o=JSON.parse('{"id":"examples/guides/object-detection-example","title":"Using YOLO to track objects","description":"This example provides a use case for Object Detection Components. We show how to create a custom component which","source":"@site/docs/examples/guides/object-detection-example.md","sourceDirName":"examples/guides","slug":"/examples/guides/object-detection-example","permalink":"/docs/examples/guides/object-detection-example","draft":false,"unlisted":true,"editUrl":"https://github.com/aica-technology/api/tree/main/docs/docs/examples/guides/object-detection-example.md","tags":[],"version":"current","sidebarPosition":8,"frontMatter":{"sidebar_position":8,"title":"Using YOLO to track objects","unlisted":true},"sidebar":"examplesSidebar"}');var a=t(4848),r=t(8453);const i=t.p+"assets/images/object-detection-example-rviz-5facf8b4edb390ae037f0458d6748636.gif",s={sidebar_position:8,title:"Using YOLO to track objects",unlisted:!0},c="Using YOLO to track objects",l={},d=[{value:"Setup",id:"setup",level:2},{value:"Using the YOLO Executor",id:"using-the-yolo-executor",level:2},{value:"Running the application",id:"running-the-application",level:3},{value:"Tracking an object with YOLO",id:"tracking-an-object-with-yolo",level:2},{value:"Creating a custom component to estimate position from bounding boxes",id:"creating-a-custom-component-to-estimate-position-from-bounding-boxes",level:3},{value:"Set up the repository",id:"set-up-the-repository",level:4},{value:"Component code",id:"component-code",level:4},{value:"Application setup",id:"application-setup",level:3},{value:"Creating a frame and converting it to a signal",id:"creating-a-frame-and-converting-it-to-a-signal",level:4},{value:"Transforming a frame to world",id:"transforming-a-frame-to-world",level:4},{value:"Moving a robot towards an object",id:"moving-a-robot-towards-an-object",level:4},{value:"Application code",id:"application-code",level:3}];function h(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components},{Details:o}=n;return o||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"using-yolo-to-track-objects",children:"Using YOLO to track objects"})}),"\n",(0,a.jsxs)(n.p,{children:["This example provides a use case for ",(0,a.jsx)(n.code,{children:"Object Detection Components"}),". We show how to create a custom component which\nconverts bounding boxes into a 3D pose. The example works with any RBG ",(0,a.jsx)(n.code,{children:"sensor_msgs::msg::Image"})," signal, such as\nprovided by the Camera Streamer component from ",(0,a.jsx)(n.code,{children:"components/core-vision"}),". The final application will be able to track an\nobject with a robot based on a fixed camera position."]}),"\n",(0,a.jsx)("div",{class:"text--center",children:(0,a.jsx)("img",{src:i,alt:"Moving the robot towards an object in RViz"})}),"\n",(0,a.jsx)(n.h2,{id:"setup",children:"Setup"}),"\n",(0,a.jsxs)(n.p,{children:["To run a YOLO executor, you need a yolo model file in ONNX format and a YAML class file. Yolo models are widely\navailable in PT formats. For example, download the lightweight YOLO12n.pt from\nultralytics ",(0,a.jsx)(n.a,{href:"https://github.com/sunsmarterjie/yolov12",children:"here"}),"."]}),"\n",(0,a.jsx)(n.p,{children:"To convert a PT file to ONNX, run the following Python code with ultralytics installed:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from ultralytics import YOLO\nmodel = YOLO("yolov12n.pt")\nmodel.export(format="onnx")  # creates \'yolov12n.onnx\'\n'})}),"\n",(0,a.jsxs)(n.p,{children:["For this example, also download the standard coco.yaml class\nfile ",(0,a.jsx)(n.a,{href:"https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/datasets/coco.yaml",children:"here"}),".\nMove the ONNX model file and the YAML class file into a new directory."]}),"\n",(0,a.jsx)(n.p,{children:"In AICA Launcher, create a configuration with the following core version and packages:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"AICA Core v4.4.2"}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"collections/object-detection"})," for the YOLO Executor component (TODO: check package name post-release and specify\nversion)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"components/core-vision v1.0.0"})," for the Camera Streamer component"]}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["Under Advanced Settings, add a volume containing the folder where you stored your YOLO model files and link it to\n",(0,a.jsx)(n.code,{children:"/files"}),"."]}),"\n",(0,a.jsx)(n.h2,{id:"using-the-yolo-executor",children:"Using the YOLO Executor"}),"\n",(0,a.jsxs)(n.p,{children:["The YOLO Executor component observes runs the YOLO segmentation model on an image.\nIt takes a camera stream and outputs the segmented image as well as the locations of bounding boxes on the image. The\nuser can set the ",(0,a.jsx)(n.code,{children:"Object Class"})," parameter, which will cause the component to set the predicate ",(0,a.jsx)(n.code,{children:"Object Detected"})," to\n",(0,a.jsx)(n.code,{children:"True"})," when the specified object is found in the image. The ",(0,a.jsx)(n.code,{children:"Confidence Threshold"})," parameter is the minimum score a\npredicted bounding box must have to be considered a valid detection. ",(0,a.jsx)(n.code,{children:"IOU Threshold"})," is used during Non-Maximum\nSuppression (NMS) to decide whether two bounding boxes represent the same object. For example, if ",(0,a.jsx)(n.code,{children:"IOU threshold"})," is set\nto 0.5, any box that overlaps more than 50% with a higher-scoring box will be discarded. For now we leave them as\ndefault values."]}),"\n",(0,a.jsx)(n.p,{children:"To test the YOLO executor:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Create a new application"}),"\n",(0,a.jsx)(n.li,{children:"Remove the default Hardware Interface node"}),"\n",(0,a.jsxs)(n.li,{children:["Add the Camera Streamer component from the core vision package","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Set the ",(0,a.jsx)(n.code,{children:"Source"})," parameter to a video device or file accordingly."]}),"\n",(0,a.jsxs)(n.li,{children:["Turn on the ",(0,a.jsx)(n.strong,{children:"auto-configure"})," and ",(0,a.jsx)(n.strong,{children:"auto-activate"})," switches"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["Add the YOLO Executor component","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Set the ",(0,a.jsx)(n.code,{children:"Model Path"})," parameter to the ",(0,a.jsx)(n.code,{children:".onnx"})," file, e.g., ",(0,a.jsx)(n.code,{children:"/files/yolo12n.onnx"})]}),"\n",(0,a.jsxs)(n.li,{children:["Set the ",(0,a.jsx)(n.code,{children:"Classes Path"})," parameter to the yaml label file, e.g., ",(0,a.jsx)(n.code,{children:"/files/coco.yaml"})]}),"\n",(0,a.jsxs)(n.li,{children:["Set the ",(0,a.jsx)(n.code,{children:"Rate"})," parameter to 3 (works on most machines), on a machine with a GPU this can be set to higher."]}),"\n",(0,a.jsxs)(n.li,{children:["Turn on the ",(0,a.jsx)(n.strong,{children:"auto-configure"})," and ",(0,a.jsx)(n.strong,{children:"auto-activate"})," switches"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.li,{children:"Connect the output of the start node to each component to load them when the application is started"}),"\n",(0,a.jsxs)(n.li,{children:["Connect the ",(0,a.jsx)(n.code,{children:"Image"})," output of the Camera Streamer to the ",(0,a.jsx)(n.code,{children:"RGB Image"})," input of the YOLO Executor"]}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["The complete application is shown below:\n",(0,a.jsx)(n.img,{alt:"Graph",src:t(635).A+"",width:"1159",height:"449"})]}),"\n",(0,a.jsxs)(n.p,{children:["The YOLO Executors parameters are as follows:\n",(0,a.jsx)(n.img,{alt:"Graph",src:t(4016).A+"",width:"1042",height:"507"})]}),"\n",(0,a.jsx)(n.h3,{id:"running-the-application",children:"Running the application"}),"\n",(0,a.jsxs)(n.p,{children:["Start the application in AICA Studio. Then open ",(0,a.jsx)(n.strong,{children:"RViz"}),': bottom-right gear icon \u2192 "Launch RViz", then RViz \u2192 Add \u2192 By\ntopic \u2192 ',(0,a.jsx)(n.code,{children:"/yolo_executor/annotated_image/Image"})," to view the YOLO model's annotated output. It should show the camera\nimages with bounding boxes drawn around key objects in it. The bounding boxes are published on the\n",(0,a.jsx)(n.code,{children:"yolo_executor/bounding_boxes"})," topic as ",(0,a.jsx)(n.code,{children:"std_msgs/msg/String"}),' and can be viewed in AICA Studio in the "ROS Topics" tab.\nChanging the ',(0,a.jsx)(n.code,{children:"Confidence Threshold"})," and ",(0,a.jsx)(n.code,{children:"IOU Threshold"})," parameters will change how often and how many bounding boxes the\nmodel outputs."]}),"\n",(0,a.jsx)(n.admonition,{type:"note",children:(0,a.jsx)(n.p,{children:"Only users with a Linux host can visualize the image stream with RViz. On macOS, AICA Launcher will not show the RViz\noption."})}),"\n",(0,a.jsx)(n.h2,{id:"tracking-an-object-with-yolo",children:"Tracking an object with YOLO"}),"\n",(0,a.jsx)(n.p,{children:"The bounding boxes generated by YOLO can be used to move a robot towards an object."}),"\n",(0,a.jsx)(n.h3,{id:"creating-a-custom-component-to-estimate-position-from-bounding-boxes",children:"Creating a custom component to estimate position from bounding boxes"}),"\n",(0,a.jsxs)(n.p,{children:["We first need to create a custom component which can estimate a 3D pose from a position in the camera frame. More\ninformation about custom components can be found ",(0,a.jsx)(n.a,{href:"https://docs.aica.tech/docs/category/custom-components/",children:"here"}),". We\nmake 3 simplifying assumptions:"]}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Camera is fixed and angled downward"}),"\n",(0,a.jsxs)(n.li,{children:["Known camera position pointing down: ",(0,a.jsx)(n.code,{children:"(x=0, y=0.6, z=0.6)"})]}),"\n",(0,a.jsxs)(n.li,{children:["Objects are on a flat surface at ",(0,a.jsx)(n.code,{children:"z=0"})]}),"\n"]}),"\n",(0,a.jsx)(n.h4,{id:"set-up-the-repository",children:"Set up the repository"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["Create a git repository from the ",(0,a.jsx)(n.a,{href:"https://github.com/aica-technology/component-template",children:"component-template"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"Clone the repository, enter the directory, and run:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"./initialize_package.sh\n"})}),"\n",(0,a.jsxs)(n.p,{children:["Name it ",(0,a.jsx)(n.code,{children:"component_utils"})," and include a Python Lifecycle component"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["Rename ",(0,a.jsx)(n.code,{children:"py_lifecycle_component.py"})," to ",(0,a.jsx)(n.code,{children:"yolo_to_marker.py"})," in ",(0,a.jsx)(n.code,{children:"source/component_utils/component_utils/"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["Rename ",(0,a.jsx)(n.code,{children:"py_lifecycle_component.json"})," to ",(0,a.jsx)(n.code,{children:"yolo_to_marker.json"})," in ",(0,a.jsx)(n.code,{children:"source/component_utils/component_descriptions/"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["Register the component in ",(0,a.jsx)(n.code,{children:"source/component_utils/setup.cfg"})," under ",(0,a.jsx)(n.code,{children:"[options.entry_points]"})," like this:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-cfg",children:"component_utils::YoloToMarker = component_utils.yolo_to_marker:YoloToMarker\n"})}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h4,{id:"component-code",children:"Component code"}),"\n",(0,a.jsxs)(n.p,{children:["Below is the core implementation, which can be copied into the respective files. The component expects a JSON string as\ninput provided by the YOLO Executor component, projects bounding boxes into 3D based on camera height and field of view,\nand outputs a ",(0,a.jsx)(n.code,{children:"CartesianState"})," for robot control."]}),"\n",(0,a.jsxs)(o,{children:[(0,a.jsx)("summary",{children:" yolo_to_marker.py "}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import state_representation as sr\nfrom modulo_components.lifecycle_component import LifecycleComponent\nfrom std_msgs.msg import String\nfrom modulo_core import EncodedState\nimport clproto\nimport json\nimport numpy as np\n\nfrom rclpy.duration import Duration\nfrom copy import deepcopy\n\nclass YoloToMarker(LifecycleComponent):\n    def __init__(self, node_name: str, *args, **kwargs):\n        super().__init__(node_name, *args, **kwargs)\n        # inputs\n        self.json_input = ""\n        self.add_input("json_input", "json_input", String)\n\n        # outputs\n        self._marker_pose = sr.CartesianPose("object", "camera_frame") #pose with name object, referenced to camera frame\n        self.add_output("marker_pose", "_marker_pose", EncodedState)\n\n        # parameters\n        self.to_find = \'person\'\n        self.fov = [69, 42]\n        self.image_size = [480, 840]\n        self.object_distance = 0.6\n        \n        self.add_parameter(sr.Parameter("to_find", \'person\', sr.ParameterType.STRING), "thing to find and track")\n        self.add_parameter(sr.Parameter("fov", [69.0, 42.0], sr.ParameterType.DOUBLE_ARRAY), "Camera FoV in X")\n        self.add_parameter(sr.Parameter("image_size", [480, 840], sr.ParameterType.DOUBLE_ARRAY), "Size of image (pixels)")\n        self.add_parameter(sr.Parameter("object_distance", 0.6, sr.ParameterType.DOUBLE), "Target position (Z)")\n    \n    def on_configure_callback(self) -> bool:\n        # configuration steps before running\n        self.fov = self.get_parameter_value("fov")\n        self.fov = np.radians(np.asarray(self.fov))\n\n        self.object_distance = self.get_parameter_value("object_distance")\n        return True\n\n    def __centre_pt_to_position(self, xy):\n        # function to find find the 3d position of an object based on position in the image.\n        image_size = np.asarray(self.image_size)\n        ratio = np.clip((xy - image_size / 2) / (image_size / 2), -1, 1)\n\n        position = ratio * self.object_distance * np.tan(self.fov/2)    \n\n        position = (np.append(position, self.object_distance)).reshape((3,1))\n        return position\n    \n    def on_step_callback(self):\n        try:\n            data = json.loads(self.json_input)[\'detections\']\n            # self.get_logger().info(data)\n\n            position = {}\n            for i, detection in enumerate(data):\n                centre_of_bbox = np.asarray([\n                    (detection[\'x_min\'] + detection[\'x_max\']) / 2,\n                    (detection[\'y_min\'] + detection[\'y_max\']) / 2\n                ])\n                position_key = f"{detection[\'class_name\']}"\n                position[position_key] = self.__centre_pt_to_position(centre_of_bbox)\n            \n            self.get_logger().info(f\'{position}\')   \n\n            # output pose of target object\n            to_find = self.get_parameter_value("to_find")\n            if to_find in position:\n                self._marker_pose.set_position(position[to_find])\n\n        except Exception as e:\n            self.get_logger().error(f\'Error in on_step_callback: {e}\')\n'})})]}),"\n",(0,a.jsxs)(o,{children:[(0,a.jsx)("summary",{children:"yolo_to_marker.json"}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-json",children:'{\n  "$schema": "https://docs.aica.tech/schemas/1-1-1/component.schema.json",\n  "name": "YOLO to marker",\n  "description": {\n    "brief": "Reads bounding boxes and outputs an interactive marker",\n    "details": "This component receives YOLO object detection outputs in JSON format, identifies a specified target object, and converts its bounding box coordinates into a 3D Cartesian pose relative to the camera frame. The pose is computed assuming a fixed distance from the camera and using the camera\'s field of view and image resolution. If the target object is detected, its 3D position is output as an interactive marker pose."\n  },\n  "registration": "component_utils::YoloToMarker",\n  "inherits": "modulo_components::LifecycleComponent",\n  "inputs": [\n    {\n      "display_name": "Json Input",\n      "description": "Bounding boxes from YOLO model",\n      "signal_name": "json_input",\n      "signal_type": "string"\n    }\n  ],\n  "outputs": [\n    {\n      "display_name": "Pose Command",\n      "description": "The pose command",\n      "signal_name": "marker_pose",\n      "signal_type": "cartesian_state"\n    }\n  ],\n  "parameters": [\n    {\n      "display_name": "Object to Track",\n      "description": "The name of the object to track, matching a class from the YOLO model.",\n      "parameter_name": "to_find",\n      "parameter_type": "string",\n      "default_value": "person"\n    },\n    {\n      "display_name": "Field of View (FoV)",\n      "description": "Camera field of view in degrees [horizontal, vertical]",\n      "parameter_name": "fov",\n      "parameter_type": "double_array",\n      "default_value": [\n        69.0,\n        42.0\n      ]\n    },\n    {\n      "display_name": "Image Size",\n      "description": "Image resolution [width, height] in pixels",\n      "parameter_name": "image_size",\n      "parameter_type": "double_array",\n      "default_value": [\n        480,\n        840\n      ]\n    },\n    {\n      "display_name": "Object Distance",\n      "description": "Assumed distance from the camera to the object in meters",\n      "parameter_name": "object_distance",\n      "parameter_type": "double",\n      "default_value": 0.6,\n      "dynamic": true\n    }\n  ]\n}\n'})})]}),"\n",(0,a.jsx)(n.p,{children:"Enter the component folder in terminal and run"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"docker build -f aica-package.toml -t objectdetection .\n"})}),"\n",(0,a.jsxs)(n.p,{children:["Next, edit the AICA Launcher configuration and enter ",(0,a.jsx)(n.code,{children:"objectdetection"}),' under Custom Packages. After launching, you\nshould see\nthe Component Utils package listed in the "Add Component" menu and be able to add the custom YOLO to Marker component.']}),"\n",(0,a.jsx)(n.h3,{id:"application-setup",children:"Application setup"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Add the YOLO to Marker component to the previously configured application","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Set the ",(0,a.jsx)(n.code,{children:"Rate"})," parameter to match the rate of the YOLO Executor"]}),"\n",(0,a.jsxs)(n.li,{children:["Set the ",(0,a.jsx)(n.code,{children:"Object to Track"})," parameter to ",(0,a.jsx)(n.code,{children:"person"})," to track yourself in the frame"]}),"\n",(0,a.jsx)(n.li,{children:"Set the remaining parameters based on your camera configuration"}),"\n",(0,a.jsxs)(n.li,{children:["Turn on the ",(0,a.jsx)(n.strong,{children:"auto-configure"})," and ",(0,a.jsx)(n.strong,{children:"auto-activate"})," switches"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["Connect the ",(0,a.jsx)(n.code,{children:"Bounding Boxes"})," output of the YOLO Executor component to the ",(0,a.jsx)(n.code,{children:"JSON Input"})," input of the YOLO to Marker\ncomponent"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"With the application running the logs should show the 3D position in the camera_frame."}),"\n",(0,a.jsx)(n.h4,{id:"creating-a-frame-and-converting-it-to-a-signal",children:"Creating a frame and converting it to a signal"}),"\n",(0,a.jsxs)(n.p,{children:["Add a Hardware Interface node to the application and select the ",(0,a.jsx)(n.code,{children:"Generic six-axis robot arm"})," in the ",(0,a.jsx)(n.code,{children:"URDF"}),' selection,\nthen run the application. When it is running, select "3D Viz" at\nthe top right. Record a frame ',(0,a.jsx)(n.code,{children:"tool0"})," and name it ",(0,a.jsx)(n.code,{children:"camera_frame"}),"; this is the position of the camera. It can be moved by\ndragging the axis markers, with the Z direction as the direction in which the camera is pointing. Note that the robot\nwill move to where the objects are detected, which may be unreachable depending on the camera position, and cause the\nrobot controller to stop."]}),"\n",(0,a.jsx)(n.admonition,{type:"note",children:(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.code,{children:"tool0"})," is the end effector frame for Generic six-axis robot arm. If using a different robot then the frame should be\nchanged accordingly."]})}),"\n",(0,a.jsx)(n.p,{children:'Stop the application, and go back to "Graph", open the yaml code and check that the recorded frame is there, for\nexample:'}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:"frames:\n  camera_frame:\n    reference_frame: world\n    position:\n      x: 0\n      y: 0.3\n      z: 0.4\n    orientation:\n      w: 0\n      x: 0\n      y: 1\n      z: 0\n"})}),"\n",(0,a.jsx)(n.p,{children:"To convert the frame to a signal:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Add TF to Signal from AICA Core Components package","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Connect the load node to the On Start block (purple square with a black play symbol)"}),"\n",(0,a.jsxs)(n.li,{children:["Configure the component to ",(0,a.jsx)(n.strong,{children:"auto-configure"}),", ",(0,a.jsx)(n.strong,{children:"auto-activate"})]}),"\n",(0,a.jsxs)(n.li,{children:["Set camera_frame and Reference frame to world.\nWith the application running the camera frame can be observed under\n",(0,a.jsx)(n.code,{children:"ROS Topics/tf_to_signal/pose(modulo_interfaces/msg/EncodedState)"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.h4,{id:"transforming-a-frame-to-world",children:"Transforming a frame to world"}),"\n",(0,a.jsxs)(n.p,{children:["To drive the robot towards the object position it needs to be in world frame. The Cartesian Transformation component can\nconvert from camera_frame to world. Add it to the application and connect the load node, connect the ",(0,a.jsx)(n.code,{children:"Pose Output"})," from\nTF to Signal to ",(0,a.jsx)(n.code,{children:"Input 1"}),", and the output of YOLO to Marker to ",(0,a.jsx)(n.code,{children:"Input 2"}),". The output of Cartesian Transformation is now\nthe object position in the world frame."]}),"\n",(0,a.jsx)(n.h4,{id:"moving-a-robot-towards-an-object",children:"Moving a robot towards an object"}),"\n",(0,a.jsx)(n.p,{children:"We can use a Signal Point Attractor to move the robot end effector towards the object."}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Add the Signal Point Attractor to the application","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Configure the component to ",(0,a.jsx)(n.strong,{children:"auto-configure"})," and ",(0,a.jsx)(n.strong,{children:"auto-activate"}),"."]}),"\n",(0,a.jsx)(n.li,{children:"Connect the load node to the On Activate transition in YOLO to Marker. This means that the robot will not move\nuntil after YOLO to Marker has started."}),"\n",(0,a.jsxs)(n.li,{children:["Connect ",(0,a.jsx)(n.code,{children:"Cartesian state"}),", under Robot State Broadcaster to the ",(0,a.jsx)(n.code,{children:"Input pose"})," of Signal Point Attractor."]}),"\n",(0,a.jsxs)(n.li,{children:["Connect the output from Cartesian Transformation to ",(0,a.jsx)(n.code,{children:"Attractor pose"}),"."]}),"\n",(0,a.jsxs)(n.li,{children:["Connect ",(0,a.jsx)(n.code,{children:"Output twist"})," to an IK Velocity Controller on the Hardware Interface, this can be added under\nControllers."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"The final graph is shown below:"}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{alt:"Graph",src:t(4505).A+"",width:"1722",height:"758"})}),"\n",(0,a.jsx)(n.h3,{id:"application-code",children:"Application code"}),"\n",(0,a.jsxs)(o,{children:[(0,a.jsx)("summary",{children:"yaml application"}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:"schema: 2-0-4\ndependencies:\n  core: v4.2.0\nframes:\n  camera_frame:\n    reference_frame: world\n    position:\n      x: 0\n      y: 0.3\n      z: 0.4\n    orientation:\n      w: 0\n      x: 0\n      y: 1\n      z: 0\non_start:\n  load:\n    - hardware: hardware\n    - component: yolo_executor\n    - component: camera_streamer\n    - component: tf_to_signal\n    - component: yolo_to_marker\ncomponents:\n  yolo_to_marker:\n    component: component_utils::YoloToMarker\n    display_name: YOLO to marker\n    events:\n      transitions:\n        on_load:\n          lifecycle:\n            component: yolo_to_marker\n            transition: configure\n        on_configure:\n          lifecycle:\n            component: yolo_to_marker\n            transition: activate\n        on_activate:\n          load:\n            component: signal_point_attractor\n    parameters:\n      rate: !!float 5.0\n      object_distance: 0.4\n    inputs:\n      json_input: /yolo_executor/bounding_boxes\n    outputs:\n      marker_pose: /yolo_to_marker/state_command\n  signal_point_attractor:\n    component: aica_core_components::motion::SignalPointAttractor\n    display_name: Signal Point Attractor\n    events:\n      transitions:\n        on_configure:\n          lifecycle:\n            component: signal_point_attractor\n            transition: activate\n        on_load:\n          lifecycle:\n            component: signal_point_attractor\n            transition: configure\n    parameters:\n      rate: !!float 2.0\n    inputs:\n      state: /hardware/robot_state_broadcaster/cartesian_state\n      attractor: /cartesian_transformation/output\n    outputs:\n      twist: /signal_point_attractor/twist\n  cartesian_transformation:\n    component: aica_core_components::signal::CartesianTransformation\n    display_name: Cartesian Transformation\n    inputs:\n      input_1: /tf_to_signal/pose\n      input_2: /yolo_to_marker/state_command\n    outputs:\n      output: /cartesian_transformation/output\n  tf_to_signal:\n    component: aica_core_components::ros::TfToSignal\n    display_name: TF to Signal\n    events:\n      transitions:\n        on_configure:\n          lifecycle:\n            component: tf_to_signal\n            transition: activate\n        on_load:\n          lifecycle:\n            component: tf_to_signal\n            transition: configure\n          load:\n            component: cartesian_transformation\n    parameters:\n      frame: camera_frame\n    outputs:\n      pose: /tf_to_signal/pose\n  camera_streamer:\n    component: core_vision_components::image_streaming::CameraStreamer\n    display_name: Camera Streamer\n    events:\n      transitions:\n        on_load:\n          lifecycle:\n            component: camera_streamer\n            transition: configure\n        on_configure:\n          lifecycle:\n            component: camera_streamer\n            transition: activate\n    parameters:\n      source: /files/videos/hand.mp4\n      undistorted_image_cropping: false\n    outputs:\n      image: /camera_streamer/image\n  yolo_executor:\n    component: object_detection_components::YOLOExecutor\n    display_name: YOLO Executor\n    events:\n      transitions:\n        on_load:\n          lifecycle:\n            component: yolo_executor\n            transition: configure\n        on_configure:\n          lifecycle:\n            component: yolo_executor\n            transition: activate\n    parameters:\n      rate: !!float 1.0\n      model_file: /files/onnx/best.onnx\n      classes_file: /files/onnx/coco8.yaml\n    inputs:\n      rgb_image: /camera_streamer/image\n    outputs:\n      bounding_boxes: /yolo_executor/bounding_boxes\nhardware:\n  hardware:\n    display_name: Hardware Interface\n    urdf: Generic six-axis robot arm\n    rate: 100\n    events:\n      transitions:\n        on_load:\n          load:\n            - controller: robot_state_broadcaster\n              hardware: hardware\n            - controller: ik_velocity_controller\n              hardware: hardware\n    controllers:\n      robot_state_broadcaster:\n        plugin: aica_core_controllers/RobotStateBroadcaster\n        outputs:\n          cartesian_state: /hardware/robot_state_broadcaster/cartesian_state\n        events:\n          transitions:\n            on_load:\n              switch_controllers:\n                hardware: hardware\n                activate: robot_state_broadcaster\n      ik_velocity_controller:\n        plugin: aica_core_controllers/velocity/IKVelocityController\n        inputs:\n          command: /signal_point_attractor/twist\n        events:\n          transitions:\n            on_load:\n              switch_controllers:\n                hardware: hardware\n                activate: ik_velocity_controller\ngraph:\n  positions:\n    on_start:\n      x: 300\n      y: 0\n    stop:\n      x: 300\n      y: 100\n    components:\n      yolo_to_marker:\n        x: 1440\n        y: 60\n      signal_point_attractor:\n        x: 1460\n        y: 520\n      cartesian_transformation:\n        x: 960\n        y: 600\n      tf_to_signal:\n        x: 460\n        y: 560\n      camera_streamer:\n        x: 520\n        y: 160\n      yolo_executor:\n        x: 1020\n        y: 100\n    hardware:\n      hardware:\n        x: 1960\n        y: -20\n  edges:\n    yolo_to_marker_state_command_cartesian_transformation_input_2:\n      path:\n        - x: 1860\n          y: 280\n        - x: 1860\n          y: 580\n        - x: 940\n          y: 580\n        - x: 940\n          y: 860\n    yolo_executor_on_activate_yolo_to_marker_yolo_to_marker:\n      path:\n        - x: 1380\n          y: 200\n        - x: 1380\n          y: 120\n    on_start_on_start_yolo_executor_yolo_executor:\n      path:\n        - x: 680\n          y: 40\n        - x: 680\n          y: 160\n    on_start_on_start_camera_streamer_camera_streamer:\n      path:\n        - x: 480\n          y: 40\n        - x: 480\n          y: 220\n    hardware_hardware_robot_state_broadcaster_cartesian_state_signal_point_attractor_state:\n      path:\n        - x: 1400\n          y: 520\n        - x: 1400\n          y: 780\n    camera_streamer_image_yolo_executor_rgb_image:\n      path:\n        - x: 980\n          y: 420\n        - x: 980\n          y: 360\n    on_start_on_start_tf_to_signal_tf_to_signal:\n      path:\n        - x: 440\n          y: 40\n        - x: 440\n          y: 620\n    on_start_on_start_yolo_to_marker_yolo_to_marker:\n      path:\n        - x: 920\n          y: 40\n        - x: 920\n          y: 120\n    yolo_to_marker_on_activate_signal_point_attractor_signal_point_attractor:\n      path:\n        - x: 1900\n          y: 240\n        - x: 1900\n          y: 540\n        - x: 1420\n          y: 540\n        - x: 1420\n          y: 580\n    tf_to_signal_on_load_cartesian_transformation_cartesian_transformation:\n      path:\n        - x: 880\n          y: 740\n        - x: 880\n          y: 660\n    yolo_executor_bounding_boxes_yolo_to_marker_json_input:\n      path:\n        - x: 1420\n          y: 360\n        - x: 1420\n          y: 320\n    yolo_to_marker_marker_pose_cartesian_transformation_input_2:\n      path:\n        - x: 1860\n          y: 320\n        - x: 1860\n          y: 500\n        - x: 920\n          y: 500\n        - x: 920\n          y: 860\n"})})]})]})}function p(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(h,{...e})}):h(e)}},4016:(e,n,t)=>{t.d(n,{A:()=>o});const o=t.p+"assets/images/object-detection-yolo-parameters-f184ad70c6266b413362e42bfeaddf6c.png"},4505:(e,n,t)=>{t.d(n,{A:()=>o});const o=t.p+"assets/images/object-detection-example-graph-aaeb6c15230719d7178e5b8693084dd5.png"},8453:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>s});var o=t(6540);const a={},r=o.createContext(a);function i(e){const n=o.useContext(r);return o.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:i(e.components),o.createElement(r.Provider,{value:n},e.children)}}}]);