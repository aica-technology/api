"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[9182],{5537:(e,n,t)=>{t.d(n,{A:()=>w});var o=t(6540),a=t(8215),r=t(5627),i=t(6347),s=t(372),c=t(604),l=t(1861),d=t(8749);function h(e){return o.Children.toArray(e).filter((e=>"\n"!==e)).map((e=>{if(!e||(0,o.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)}))?.filter(Boolean)??[]}function u(e){const{values:n,children:t}=e;return(0,o.useMemo)((()=>{const e=n??function(e){return h(e).map((e=>{let{props:{value:n,label:t,attributes:o,default:a}}=e;return{value:n,label:t,attributes:o,default:a}}))}(t);return function(e){const n=(0,l.XI)(e,((e,n)=>e.value===n.value));if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map((e=>e.value)).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e}),[n,t])}function m(e){let{value:n,tabValues:t}=e;return t.some((e=>e.value===n))}function p(e){let{queryString:n=!1,groupId:t}=e;const a=(0,i.W6)(),r=function(e){let{queryString:n=!1,groupId:t}=e;if("string"==typeof n)return n;if(!1===n)return null;if(!0===n&&!t)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return t??null}({queryString:n,groupId:t});return[(0,c.aZ)(r),(0,o.useCallback)((e=>{if(!r)return;const n=new URLSearchParams(a.location.search);n.set(r,e),a.replace({...a.location,search:n.toString()})}),[r,a])]}function f(e){const{defaultValue:n,queryString:t=!1,groupId:a}=e,r=u(e),[i,c]=(0,o.useState)((()=>function(e){let{defaultValue:n,tabValues:t}=e;if(0===t.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(n){if(!m({value:n,tabValues:t}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${n}" but none of its children has the corresponding value. Available values are: ${t.map((e=>e.value)).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return n}const o=t.find((e=>e.default))??t[0];if(!o)throw new Error("Unexpected error: 0 tabValues");return o.value}({defaultValue:n,tabValues:r}))),[l,h]=p({queryString:t,groupId:a}),[f,x]=function(e){let{groupId:n}=e;const t=function(e){return e?`docusaurus.tab.${e}`:null}(n),[a,r]=(0,d.Dv)(t);return[a,(0,o.useCallback)((e=>{t&&r.set(e)}),[t,r])]}({groupId:a}),g=(()=>{const e=l??f;return m({value:e,tabValues:r})?e:null})();(0,s.A)((()=>{g&&c(g)}),[g]);return{selectedValue:i,selectValue:(0,o.useCallback)((e=>{if(!m({value:e,tabValues:r}))throw new Error(`Can't select invalid tab value=${e}`);c(e),h(e),x(e)}),[h,x,r]),tabValues:r}}var x=t(9136);const g={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var y=t(4848);function b(e){let{className:n,block:t,selectedValue:o,selectValue:i,tabValues:s}=e;const c=[],{blockElementScrollPositionUntilNextRender:l}=(0,r.a_)(),d=e=>{const n=e.currentTarget,t=c.indexOf(n),a=s[t].value;a!==o&&(l(n),i(a))},h=e=>{let n=null;switch(e.key){case"Enter":d(e);break;case"ArrowRight":{const t=c.indexOf(e.currentTarget)+1;n=c[t]??c[0];break}case"ArrowLeft":{const t=c.indexOf(e.currentTarget)-1;n=c[t]??c[c.length-1];break}}n?.focus()};return(0,y.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,a.A)("tabs",{"tabs--block":t},n),children:s.map((e=>{let{value:n,label:t,attributes:r}=e;return(0,y.jsx)("li",{role:"tab",tabIndex:o===n?0:-1,"aria-selected":o===n,ref:e=>{c.push(e)},onKeyDown:h,onClick:d,...r,className:(0,a.A)("tabs__item",g.tabItem,r?.className,{"tabs__item--active":o===n}),children:t??n},n)}))})}function _(e){let{lazy:n,children:t,selectedValue:r}=e;const i=(Array.isArray(t)?t:[t]).filter(Boolean);if(n){const e=i.find((e=>e.props.value===r));return e?(0,o.cloneElement)(e,{className:(0,a.A)("margin-top--md",e.props.className)}):null}return(0,y.jsx)("div",{className:"margin-top--md",children:i.map(((e,n)=>(0,o.cloneElement)(e,{key:n,hidden:e.props.value!==r})))})}function j(e){const n=f(e);return(0,y.jsxs)("div",{className:(0,a.A)("tabs-container",g.tabList),children:[(0,y.jsx)(b,{...n,...e}),(0,y.jsx)(_,{...n,...e})]})}function w(e){const n=(0,x.A)();return(0,y.jsx)(j,{...e,children:h(e.children)},String(n))}},8453:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>s});var o=t(6540);const a={},r=o.createContext(a);function i(e){const n=o.useContext(r);return o.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:i(e.components),o.createElement(r.Provider,{value:n},e.children)}},9329:(e,n,t)=>{t.d(n,{A:()=>i});t(6540);var o=t(8215);const a={tabItem:"tabItem_Ymn6"};var r=t(4848);function i(e){let{children:n,hidden:t,className:i}=e;return(0,r.jsx)("div",{role:"tabpanel",className:(0,o.A)(a.tabItem,i),hidden:t,children:n})}},9386:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>g,contentTitle:()=>x,default:()=>_,frontMatter:()=>f,metadata:()=>o,toc:()=>y});const o=JSON.parse('{"id":"examples/guides/yolo-example","title":"Using YOLO to track objects","description":"YOLO (You Only Look Once) is a real-time object detection algorithm that predicts bounding boxes and class probabilities","source":"@site/core/examples/guides/yolo-example.md","sourceDirName":"examples/guides","slug":"/examples/guides/yolo-example","permalink":"/core/examples/guides/yolo-example","draft":false,"unlisted":false,"editUrl":"https://github.com/aica-technology/api/tree/main/docs/core/examples/guides/yolo-example.md","tags":[],"version":"current","sidebarPosition":14,"frontMatter":{"sidebar_position":14,"title":"Using YOLO to track objects"},"sidebar":"examplesSidebar","previous":{"title":"Camera calibration","permalink":"/core/examples/guides/camera-calibration"},"next":{"title":"Core Components","permalink":"/core/category/core-components"}}');var a=t(4848),r=t(8453);const i=t.p+"assets/images/object-detection-example-app-99beb663b41847ff32664b84a0dad3d7.gif",s=t.p+"assets/images/object-detection-yolo-executor-f61cca4edd481f1571ff38e1c9b21316.png",c=t.p+"assets/images/object-detection-yolo-executor-parameters-86f3ddcb9b11bff0669e0207fad281c3.png",l=t.p+"assets/images/object-detection-robot-control-c10fc7efa2cc7bbf55851fbe01032beb.png",d=t.p+"assets/images/launcher-toolkits-cpu-8fbc7b13afde3ad38869bcb18b9763dd.png",h=t.p+"assets/images/launcher-toolkits-gpu-2682b2e3729cf79cf4270b74fa87a02c.png",u=t.p+"assets/images/launcher-enable-gpu-9d9cb4a61335047222da110b95bd69a4.png";var m=t(5537),p=t(9329);const f={sidebar_position:14,title:"Using YOLO to track objects"},x="Using YOLO to track objects",g={},y=[{value:"A YOLO example using the AICA framework",id:"a-yolo-example-using-the-aica-framework",level:2},{value:"Setup",id:"setup",level:2},{value:"Data folder",id:"data-folder",level:3},{value:"Camera calibration (optional)",id:"camera-calibration-optional",level:3},{value:"Obtaining YOLO inference models",id:"obtaining-yolo-inference-models",level:3},{value:"Class file",id:"class-file",level:3},{value:"AICA Launcher configuration",id:"aica-launcher-configuration",level:2},{value:"Using the <code>YoloExecutor</code>",id:"using-the-yoloexecutor",level:2},{value:"Running the application",id:"running-the-application",level:3},{value:"Tracking an object with YOLO",id:"tracking-an-object-with-yolo",level:2},{value:"Creating a custom twist generator component",id:"creating-a-custom-twist-generator-component",level:3},{value:"Set up the repository",id:"set-up-the-repository",level:4},{value:"Component code",id:"component-code",level:4},{value:"Application setup",id:"application-setup",level:3},{value:"Commanding the robot with the generated twist",id:"commanding-the-robot-with-the-generated-twist",level:4},{value:"Application code",id:"application-code",level:3}];function b(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components},{Details:t}=n;return t||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"using-yolo-to-track-objects",children:"Using YOLO to track objects"})}),"\n",(0,a.jsx)(n.p,{children:"YOLO (You Only Look Once) is a real-time object detection algorithm that predicts bounding boxes and class probabilities\ndirectly from an image in a single pass through a neural network. Unlike older methods that scan an image multiple times,\nYOLO processes the entire image at once, making it extremely fast and well-suited for many applications, including\nrobotics."}),"\n",(0,a.jsx)(n.h2,{id:"a-yolo-example-using-the-aica-framework",children:"A YOLO example using the AICA framework"}),"\n",(0,a.jsxs)(n.p,{children:["This page details how to run a ",(0,a.jsx)(n.code,{children:"YoloExecutor"})," component, i.e., a component that can use various YOLO models for\ninference. It also demonstrates how it could be used as part of an AICA application. In the following paragraphs, we\nshow how to create a custom component which makes use of a bounding box to adapt an arm's motion such that it maintains\nthe object centered. The ",(0,a.jsx)(n.code,{children:"YoloExecutor"})," component that is covered in following sections can be found under\n",(0,a.jsx)(n.code,{children:"components/advanced-perception"})," with a valid AICA license."]}),"\n",(0,a.jsx)("div",{class:"text--center",children:(0,a.jsx)("img",{src:i,alt:"Moving the robot towards an object in RViz"})}),"\n",(0,a.jsx)(n.h2,{id:"setup",children:"Setup"}),"\n",(0,a.jsx)(n.h3,{id:"data-folder",children:"Data folder"}),"\n",(0,a.jsxs)(n.p,{children:["Create a directory with a name of your choice, say ",(0,a.jsx)(n.code,{children:"yolo-example-data"}),", folder anywhere in your filesystem. Here, you\nwill be placing files that are required for runtime use. In following steps, you will configure AICA Launcher to mount\nthis directory for further use."]}),"\n",(0,a.jsx)(n.h3,{id:"camera-calibration-optional",children:"Camera calibration (optional)"}),"\n",(0,a.jsx)(n.p,{children:"For the purposes of this example, a camera calibration is not strictly required, but may be needed if you are using a\nhigh-distortion or fish-eye lens camera and do not have access to your camera's intrinsic parameters."}),"\n",(0,a.jsxs)(n.p,{children:["If this is your case, follow the ",(0,a.jsx)(n.a,{href:"/core/examples/guides/camera-calibration",children:"camera calibration example"})," to generate a camera intrinsics\nfile that you can then use with the ",(0,a.jsx)(n.code,{children:"CameraStreamer"}),"."]}),"\n",(0,a.jsx)(n.h3,{id:"obtaining-yolo-inference-models",children:"Obtaining YOLO inference models"}),"\n",(0,a.jsxs)(n.p,{children:["The ",(0,a.jsx)(n.code,{children:"YoloExecutor"})," component works with ",(0,a.jsx)(n.code,{children:".onnx"})," model files. However, many of the available YOLO models are widely\navailable in Pytorch (",(0,a.jsx)(n.code,{children:".pt"}),") format instead. To convert between formats, you can use AICA's utilities to do so within\na Docker container and maintain your host system unpolluted."]}),"\n",(0,a.jsx)(n.p,{children:"First, clone our docker image repository (if you followed the calibration section, you should already have it!):"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-shell",children:"git clone https://github.com/aica-technology/docker-images.git && cd docker-images\n"})}),"\n",(0,a.jsx)(n.p,{children:"then:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-shell",children:"cd yolo_model_converter\n./build-run.sh\n"})}),"\n",(0,a.jsxs)(n.p,{children:["By default, this will download and convert ",(0,a.jsx)(n.code,{children:"yolo12n"})," for you (see ",(0,a.jsx)(n.a,{href:"https://github.com/sunsmarterjie/yolov12",children:"here"})," for\nmore models). If you wish to specify one of the other models that Ultralytics is offering, simply specify it as follows:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-shell",children:"./build-run.sh --model yoloZZZZ\n"})}),"\n",(0,a.jsxs)(n.p,{children:["Once the script exits, copy the ",(0,a.jsx)(n.code,{children:".onnx"})," file that was generated in your ",(0,a.jsx)(n.code,{children:"yolo-example-data"})," directory."]}),"\n",(0,a.jsxs)(n.admonition,{type:"note",children:[(0,a.jsxs)(n.p,{children:["If you have ",(0,a.jsx)(n.code,{children:"ultralytics"})," installed in your Python environment or you are willing to install it, then download any of\nthe models available ",(0,a.jsx)(n.a,{href:"https://github.com/sunsmarterjie/yolov12",children:"here"}),". For the purposes of this example, you can opt\nfor a smaller model such as the ",(0,a.jsx)(n.code,{children:"yolo12n"}),"."]}),(0,a.jsxs)(n.p,{children:["To convert a ",(0,a.jsx)(n.code,{children:".pt"})," file to ",(0,a.jsx)(n.code,{children:".onnx"}),", run the following Python code with ",(0,a.jsx)(n.code,{children:"ultralytics"})," installed:"]}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from ultralytics import YOLO\nmodel = YOLO("yolo12n.pt")\nmodel.export(format="onnx")  # creates \'yolo12n.onnx\'\n'})}),(0,a.jsx)(n.p,{children:"This approach is also necessary if you are using custom models instead of the ones distributed by Ultralytics."})]}),"\n",(0,a.jsx)(n.h3,{id:"class-file",children:"Class file"}),"\n",(0,a.jsxs)(n.p,{children:["Along with the YOLO model from the previous paragraph, you will also need a ",(0,a.jsx)(n.strong,{children:"class file"})," (for example, ",(0,a.jsx)(n.code,{children:"coco.yaml"}),")\nthat maps the numeric class IDs predicted by the model to their corresponding class names. This file is specific to the\nmodel you are using and is typically defined during training, meaning that the mapping between class IDs and labels is\nfixed once the model is trained."]}),"\n",(0,a.jsx)(n.p,{children:"Modifying the class file after training will not affect the model's behavior or predictions. It will only change the\ntext labels displayed in your annotations. As a result, manual edits are generally safe but may lead to semantic\ninconsistencies if the names no longer match the model's intended classes."}),"\n",(0,a.jsxs)(n.p,{children:["For the purposes of this example, download the standard ",(0,a.jsx)(n.code,{children:"coco.yaml"})," class\nfile ",(0,a.jsx)(n.a,{href:"https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/datasets/coco.yaml",children:"here"})," and move it to your\n",(0,a.jsx)(n.code,{children:"yolo-example-data"})," folder, where you also stored your YOLO model."]}),"\n",(0,a.jsx)(n.h2,{id:"aica-launcher-configuration",children:"AICA Launcher configuration"}),"\n",(0,a.jsx)(n.p,{children:"In AICA Launcher, create a configuration with the following core version and packages:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"AICA Launcher v1.4.1 or higher"}),"\n",(0,a.jsx)(n.li,{children:"AICA Core v5.0.0 or higher"}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"advanced_perception"})," v1.0.0 or higher for the ",(0,a.jsx)(n.code,{children:"YoloExecutor"})," component"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"core-vision"})," v1.1.1 or higher for the ",(0,a.jsx)(n.code,{children:"CameraStreamer"})," component"]}),"\n",(0,a.jsxs)(n.li,{children:["CPU or GPU toolkit at v1.0.0 (subject to change in newer versions of ",(0,a.jsx)(n.code,{children:"core-vision"})," and/or ",(0,a.jsx)(n.code,{children:"advanced-perception"}),")"]}),"\n"]}),"\n",(0,a.jsxs)(n.admonition,{type:"info",children:[(0,a.jsx)(n.p,{children:"AICA toolkits are the curated way of bundling Machine Learning (ML) and GPU (specifically CUDA) acceleration libraries.\nIn short:"}),(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"ML toolkits contain a broad range of libraries that are often required to conduct ML inference and/or training\n(e.g., pytorch, scipy, etc)"}),"\n",(0,a.jsx)(n.li,{children:"CUDA toolkits contain libraries pertinent to interface CUDA-compatible code and libraries\nwith a NVIDIA GPU."}),"\n"]}),(0,a.jsx)(n.p,{children:"If you do not own a GPU or want CPU accleration only, bundling our CUDA toolkits is not necessary. For instance, your\nAICA Launcher configuration could look as follows:"}),(0,a.jsxs)(m.A,{groupId:"toolkits",children:[(0,a.jsx)(p.A,{value:"cpu",label:"CPU",children:(0,a.jsx)("div",{class:"text--center",children:(0,a.jsx)("img",{src:d,alt:"AICA Launcher configuration for CPU-only runtime"})})}),(0,a.jsxs)(p.A,{value:"gpu",label:"GPU",children:[(0,a.jsx)("div",{class:"text--center",children:(0,a.jsx)("img",{src:h,alt:"AICA Launcher configuration for CPU and GPU runtime"})}),(0,a.jsxs)(n.p,{children:["When using the CUDA toolkit, do not forget to enable GPU capabilities under the ",(0,a.jsx)(n.strong,{children:"Advanced Settings"})," menu:"]}),(0,a.jsx)("div",{class:"text--center",children:(0,a.jsx)("img",{src:u,alt:"AICA Launcher configuration with GPU capabilities"})})]})]})]}),"\n",(0,a.jsxs)(n.p,{children:["Finally, you need to link your configuration to the ",(0,a.jsx)(n.code,{children:"yolo-example-data"})," directory we created in earlier steps. While\nyou are still at the ",(0,a.jsx)(n.strong,{children:"Advanced Settings"})," menu:"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Click on ",(0,a.jsx)(n.strong,{children:"Add a volume mount +"}),"."]}),"\n",(0,a.jsxs)(n.li,{children:["Click on ",(0,a.jsx)(n.strong,{children:"Browse"})," and navigate to the location of the ",(0,a.jsx)(n.code,{children:"yolo-example-data"})," folder."]}),"\n",(0,a.jsxs)(n.li,{children:["On the right side, where a ",(0,a.jsx)(n.code,{children:"/target"})," placeholder text is visible, type a name for the target directory inside your\nAICA container. For simplicity you can use ",(0,a.jsx)(n.code,{children:"/yolo-example-data"}),"."]}),"\n"]}),"\n",(0,a.jsxs)(n.admonition,{type:"info",children:[(0,a.jsx)(n.p,{children:"Remember, AICA Launcher starts Docker containers with your selected configuration of packages, versions, advanced\noptions, and volume mounts. Unless you explicitly specify volume mappings from your host system to the container, the\ncontainer will not have access to the host filesystem."}),(0,a.jsxs)(n.p,{children:["An exception to this is the ",(0,a.jsx)(n.code,{children:"Data Folder"})," that is prefilled by default, is created for you automatically (host), and is\nmapped to ",(0,a.jsx)(n.code,{children:"/data"})," internally. This folder contains the AICA database that preserves your applications and settings, but\ncan also be used to persistently store data, same as custom volume mounts."]})]}),"\n",(0,a.jsxs)(n.p,{children:["Press ",(0,a.jsx)(n.strong,{children:"Launch AICA Studio."})]}),"\n",(0,a.jsxs)(n.h2,{id:"using-the-yoloexecutor",children:["Using the ",(0,a.jsx)(n.code,{children:"YoloExecutor"})]}),"\n",(0,a.jsx)(n.p,{children:"Let us build a YOLO application from scratch."}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Create a new application"}),"\n",(0,a.jsx)(n.li,{children:"Remove the default Hardware Interface node for now"}),"\n",(0,a.jsxs)(n.li,{children:["Add the Camera Streamer component from the core vision package","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Set the ",(0,a.jsx)(n.code,{children:"Source"})," parameter to a video device or file accordingly"]}),"\n",(0,a.jsxs)(n.li,{children:["Enable ",(0,a.jsx)(n.strong,{children:"auto-configure"})," and ",(0,a.jsx)(n.strong,{children:"auto-activate"})]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["Add the ",(0,a.jsx)(n.code,{children:"YoloExecutor"})," component","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Set the ",(0,a.jsx)(n.code,{children:"Model path"})," parameter to the ",(0,a.jsx)(n.code,{children:".onnx"})," file, e.g., ",(0,a.jsx)(n.code,{children:"/yolo-example-data/yolo12n.onnx"})]}),"\n",(0,a.jsxs)(n.li,{children:["Set the ",(0,a.jsx)(n.code,{children:"Classes path"})," parameter to the yaml label file, e.g., ",(0,a.jsx)(n.code,{children:"/yolo-example-data/coco.yaml"})]}),"\n",(0,a.jsxs)(n.li,{children:["Enable ",(0,a.jsx)(n.strong,{children:"auto-configure"})," and ",(0,a.jsx)(n.strong,{children:"auto-activate"})]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.li,{children:"Connect the output of the start node to each component to load them when the application is started"}),"\n",(0,a.jsxs)(n.li,{children:["Connect the ",(0,a.jsx)(n.code,{children:"Image"})," output of the Camera Streamer to the ",(0,a.jsx)(n.code,{children:"Image"})," input of the ",(0,a.jsx)(n.code,{children:"YoloExecutor"})]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"Additional parameters can be used to tune the performance of YoloExecutor (both in the computational and prediction\nsense). The following picture shows the available parameters:"}),"\n",(0,a.jsx)("div",{class:"text--center",children:(0,a.jsx)("img",{src:c,alt:"Overview of a YoloExecutor parameters"})}),"\n",(0,a.jsx)(n.p,{children:"More specifically, you can adapt:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"Model path"}),": filepath to your YOLO model"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"Classes path"}),": filepath to your classes file, making the mapping between predicted object IDs and object names"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"Object class"}),": will narrow the ",(0,a.jsx)(n.code,{children:"Detections"})," output to the selected classes alone (one or more classes included in the\nclass file)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"Confidence threshold"}),": the minimum score a predicted bounding box must have to be considered a valid detection"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"IOU Threshold"}),": used during Non-Maximum Suppression (NMS) to decide whether two bounding boxes represent the same\nobject. For example, if ",(0,a.jsx)(n.code,{children:"IOU threshold"})," is set to 0.5, any box that overlaps more than 50% with a higher-scoring box\nwill be discarded."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"Prefer GPU"}),": sets GPU as the preferred inference device. However, if you use a CPU toolkit image with ",(0,a.jsx)(n.code,{children:"Prefer GPU"}),"\ntoggled on, then the component will ultimately gracefully fall back to using the CPU"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"Number of CPU threads"}),": to get the most out of your system's resources. Notice that this parameter has no effect when\na GPU is used"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"To complement the parameters and enable event-driven logic when using the component, two predicates exist, namely:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"Is any selected object detected"}),": True if one of the objects in the ",(0,a.jsx)(n.code,{children:"Object class"})," list is detected"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.code,{children:"Is any object detected"}),": True if there is any known object (i.e., as per the class file provided) in the image stream\n(including but not limited to ",(0,a.jsx)(n.code,{children:"Object class"}),")"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"Your application should now look similar to the following picture:"}),"\n",(0,a.jsx)("div",{class:"text--center",children:(0,a.jsx)("img",{src:s,alt:"Overview of a YoloExecutor application"})}),"\n",(0,a.jsx)(n.h3,{id:"running-the-application",children:"Running the application"}),"\n",(0,a.jsx)(n.p,{children:"Open the application we built in the previous step, if you are not already there. Then:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["open ",(0,a.jsx)(n.strong,{children:"RViz"}),": from the bottom-right gear icon ",(0,a.jsx)(n.strong,{children:"\u2192"}),' "Launch RViz"']}),"\n",(0,a.jsxs)(n.li,{children:["in ",(0,a.jsx)(n.strong,{children:"RViz"}),": press Add ",(0,a.jsx)(n.strong,{children:"\u2192"})," By topic ",(0,a.jsx)(n.strong,{children:"\u2192"})," ",(0,a.jsx)(n.code,{children:"/yolo_executor/annotated_image/Image"})," to view the YOLO model's annotated\noutput. It should show the camera images with bounding boxes drawn around key objects. The bounding boxes are\npublished on the ",(0,a.jsx)(n.code,{children:"yolo_executor/detections"})," topic as ",(0,a.jsx)(n.code,{children:"vision_msgs/msg/Detection2DArray"}),", a ROS perception message (e.g.,\ncontaining bounding box coordinates, class name, score, ...)."]}),"\n"]}),"\n",(0,a.jsx)(n.admonition,{type:"note",children:(0,a.jsx)(n.p,{children:"Only users with a Linux host can visualize the image stream with RViz. On macOS, AICA Launcher will not show the RViz\noption."})}),"\n",(0,a.jsx)(n.h2,{id:"tracking-an-object-with-yolo",children:"Tracking an object with YOLO"}),"\n",(0,a.jsxs)(n.p,{children:["The bounding boxes generated by YOLO can be used to move a robot towards an object. Let us take an example were we will\nemulate a camera mounted on a robot arm and we want to command the robot such that it tries to maintain a\nselected object in the middle of the image frame. For simplicity, we specify a single object class for the ",(0,a.jsx)(n.code,{children:"YoloExecutor"}),"\nto detect, and assume that only one object of the type can appear in the image at any time."]}),"\n",(0,a.jsx)(n.h3,{id:"creating-a-custom-twist-generator-component",children:"Creating a custom twist generator component"}),"\n",(0,a.jsxs)(n.p,{children:["We first need to create a custom component that given a bounding box of an item will generate a twist indicating where\nthe frame should move to keep the object centered. More information about custom components can be found\n",(0,a.jsx)(n.a,{href:"https://docs.aica.tech/docs/category/custom-components/",children:"here"}),". The following component has been designed with regular\noff-the-self webcams and the standard YOLO models in mind, meaning:"]}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"The object orientation is not taken into account when generating a twist. That is, no angular velocity is generated."}),"\n",(0,a.jsx)(n.li,{children:"No depth information is available and/or considered. The component operates in pixel space and is invariant to an\nobject's movement along the depth axis. That is, only 2D motion will be observed, and the depth axis will have zero\nvelocity."}),"\n"]}),"\n",(0,a.jsx)(n.h4,{id:"set-up-the-repository",children:"Set up the repository"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["Create a git repository from the ",(0,a.jsx)(n.a,{href:"https://github.com/aica-technology/package-template",children:"package-template"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsx)(n.p,{children:"Clone the repository, enter the directory, and run:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"./initialize_package.sh\n"})}),"\n",(0,a.jsxs)(n.p,{children:["Name it ",(0,a.jsx)(n.code,{children:"object_detection_utils"})," and include a Python Lifecycle component"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["Rename ",(0,a.jsx)(n.code,{children:"py_lifecycle_component.py"})," to ",(0,a.jsx)(n.code,{children:"bounding_box_tracker.py"})," in ",(0,a.jsx)(n.code,{children:"source/component_utils/object_detection_utils/"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["Rename ",(0,a.jsx)(n.code,{children:"py_lifecycle_component.json"})," to ",(0,a.jsx)(n.code,{children:"object_detection_utils_bounding_box_tracker.json"})," in\n",(0,a.jsx)(n.code,{children:"source/component_utils/component_descriptions/"})]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["Register the component in ",(0,a.jsx)(n.code,{children:"source/component_utils/setup.cfg"})," like this:"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-cfg",children:"[options.entry_points]\npython_components =\n    object_detection_utils::BoundingBoxTracker = object_detection_utils.bounding_box_tracker:BoundingBoxTracker\n"})}),"\n",(0,a.jsx)(n.h4,{id:"component-code",children:"Component code"}),"\n",(0,a.jsxs)(n.p,{children:["As discussed, the component expects a ",(0,a.jsx)(n.code,{children:"Detection2DArray"})," as input (e.g. from ",(0,a.jsx)(n.code,{children:"YoloExecutor"}),") containing, among other\nthings, a bounding box. From that bounding box, it generates a ",(0,a.jsx)(n.code,{children:"CartesianTwist"})," which can be used to control a robot.\nBelow you will find the implementation of the component, which can be copied directly into your\n",(0,a.jsx)(n.code,{children:"bounding_box_tracker.py"}),"."]}),"\n",(0,a.jsxs)(t,{children:[(0,a.jsx)("summary",{children:"bounding_box_tracker.py"}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import numpy as np\n\nfrom rclpy.lifecycle import LifecycleState\nfrom vision_msgs.msg import Detection2DArray, Detection2D\nfrom lifecycle_msgs.msg import State as LifecycleState\n\nfrom modulo_components.lifecycle_component import LifecycleComponent\nfrom modulo_core import EncodedState\nimport state_representation as sr\n\n\nclass BoundingBoxTracker(LifecycleComponent):\n    def __init__(self, node_name: str, *args, **kwargs):\n        super().__init__(node_name, *args, **kwargs)\n        self._decay_rate: sr.Parameter = sr.Parameter("decay_rate", 1.0, sr.ParameterType.DOUBLE)\n        self._twist = sr.CartesianTwist.Zero("object", "world")\n        self._latest_twist = sr.CartesianTwist()\n        self._reference_frame: str = ""\n        self._camera_frame: str = ""\n\n        self.add_parameter(\n            sr.Parameter("image_size", [640, 480], sr.ParameterType.DOUBLE_ARRAY),\n            "Image resolution [width, height] in pixels",\n        )\n        self.add_parameter(sr.Parameter("gains", [0.0001, 0.0001], sr.ParameterType.DOUBLE_ARRAY), "Control gains (Kp)")\n        self.add_parameter(\n            sr.Parameter("deadband", [15.0, 15.0], sr.ParameterType.DOUBLE_ARRAY),\n            "Deadband for the error measurements [width, height], within which no twist is generated",\n        )\n        self.add_parameter("_decay_rate", "Exponential decay rate")\n        self.add_parameter(\n            sr.Parameter("reference_frame", sr.ParameterType.STRING),\n            "Optional reference frame for the output twist. If not set, the camera frame will be used.",\n        )\n\n        self.add_input("detections", self._on_receive_detections, Detection2DArray)\n        self.add_output("twist", "_twist", EncodedState)\n\n        self.add_tf_listener()\n\n    def on_validate_parameter_callback(self, parameter: sr.Parameter) -> bool:\n        name = parameter.get_name()\n        match name:\n            case "image_size" | "deadband" | "gains":\n                if len(parameter.get_value()) != 2:\n                    self.get_logger().error(f"{name} must be a list of two floats")\n                    return False\n                if any(g < 0 for g in parameter.get_value()):\n                    self.get_logger().error(f"{name} must be non-negative")\n                    return False\n            case "reference_frame":\n                if not parameter.is_empty() and len(parameter.get_value()) == 0:\n                    self.get_logger().error("Reference frame must be a non-empty string")\n                    return False\n            case "decay_rate":\n                if parameter.get_value() < 0:\n                    self.get_logger().error("Decay rate must be non-negative")\n                    return False\n        return True\n\n    def on_activate_callback(self) -> bool:\n        if not self.get_parameter("reference_frame").is_empty():  # type: ignore\n            self._reference_frame = self.get_parameter_value("reference_frame")\n        return True\n\n    def on_step_callback(self) -> None:\n        if len(self._camera_frame) == 0:\n            self.get_logger().debug(\n                "No detections received yet or no valid camera frame was found in the message",\n                throttle_duration_sec=3.0,\n            )\n            return\n        decay_factor = np.exp(-self._decay_rate.get_value() * self._latest_twist.get_age())\n        twist = sr.CartesianTwist.Zero("object", self._camera_frame)\n        twist.set_linear_velocity(np.array(self._latest_twist.get_linear_velocity()) * decay_factor)\n\n        if len(self._reference_frame) > 0 and not self._camera_frame == self._reference_frame:\n            try:\n                tf = self.lookup_transform(self._camera_frame, self._reference_frame)\n            except Exception as e:\n                self.get_logger().error(f"Failed to lookup transform: {e}")\n                return\n            self._twist = tf * twist\n        else:\n            self._twist = twist\n\n    def on_deactivate_callback(self) -> bool:\n        self._camera_frame = ""\n        self._reference_frame = ""\n        return True\n\n    def _on_receive_detections(self, msg: Detection2DArray):\n        if self.get_lifecycle_state().state_id != LifecycleState.PRIMARY_STATE_ACTIVE:\n            self.get_logger().debug("Component is not active. Ignoring incoming detections.", throttle_duration_sec=1.0)\n            return\n\n        image_size = self.get_parameter("image_size").get_value()  # type: ignore\n        gains = self.get_parameter("gains").get_value()  # type: ignore\n\n        if len(msg.detections) == 0:\n            self.get_logger().warn(\n                "No objects detected. Holding last known position with decay.", throttle_duration_sec=1.0\n            )\n            return\n        elif len(msg.detections) > 1:\n            self.get_logger().warn("Multiple objects detected. Tracking only the first one.", throttle_duration_sec=1.0)\n\n        obj: Detection2D = msg.detections[0]  # type: ignore\n\n        if self._camera_frame != obj.header.frame_id:\n            self._twist = sr.CartesianTwist.Zero("object", self._camera_frame)\n            self._latest_twist = sr.CartesianTwist.Zero("object", self._camera_frame)\n        self._camera_frame = obj.header.frame_id\n\n        if not self._reference_frame:\n            self._twist.set_reference_frame(self._reference_frame)\n\n        positions = {}\n        positions[f"{obj.results[0].hypothesis.class_id}"] = np.asarray(  # type: ignore\n            [obj.bbox.center.position.x, obj.bbox.center.position.y]\n        )\n\n        bbox = positions[list(positions.keys())[0]]\n        width_error = bbox[0] - image_size[0] / 2\n        height_error = bbox[1] - image_size[1] / 2\n        vx = 0.0\n        vy = 0.0\n        if abs(width_error) > self.get_parameter("deadband").get_value()[1]:  # type: ignore\n            vx = width_error * gains[0]\n        if abs(height_error) > self.get_parameter("deadband").get_value()[0]:  # type: ignore\n            vy = height_error * gains[1]\n\n        self._latest_twist.set_linear_velocity(vx, vy, 0.0)\n'})})]}),"\n",(0,a.jsx)(n.p,{children:"And its corresponding description file:"}),"\n",(0,a.jsxs)(t,{children:[(0,a.jsx)("summary",{children:"object_detection_utils_bounding_box_tracker.json"}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-json",children:'{\n  "$schema": "https://docs.aica.tech/schemas/1-1-1/component.schema.json",\n  "name": "Bounding box tracker",\n  "description": {\n    "brief": "Receive object detection messages and outputs a twist command to keep an object at the center of a camera\'s frame",\n    "details": "This component receives Detection2DArray messages and computes a twist command for the detected object to remain at the center of the frame. The Z axis is not considered (i.e., only X-Y plane commands will be issued)."\n  },\n  "registration": "object_detection_utils::BoundingBoxTracker",\n  "inherits": "modulo_components::LifecycleComponent",\n  "inputs": [\n    {\n      "display_name": "Detections",\n      "description": "Detection array containing bounding box information of detected objects",\n      "signal_name": "detections",\n      "signal_type": "other",\n      "custom_signal_type": "vision_msgs::msg::Detection2DArray"\n    }\n  ],\n  "outputs": [\n    {\n      "display_name": "Twist",\n      "description": "The twist command",\n      "signal_name": "twist",\n      "signal_type": "cartesian_twist"\n    }\n  ],\n  "parameters": [\n    {\n      "display_name": "Image size",\n      "description": "Image resolution [width, height] in pixels",\n      "parameter_name": "image_size",\n      "parameter_type": "double_array",\n      "default_value": "[640, 480]"\n    },\n    {\n      "display_name": "Control gains",\n      "description": "Control gains (Kp) for the twist command",\n      "parameter_name": "gains",\n      "parameter_type": "double_array",\n      "default_value": "[0.0001, 0.0001]"\n    },\n    {\n      "display_name": "Deadband",\n      "description": "Deadband for the error measurements, within which no twist is generated",\n      "parameter_name": "deadband",\n      "parameter_type": "double_array",\n      "default_value": "[15.0, 15.0]"\n    },\n    {\n      "display_name": "Decay rate",\n      "description": "Exponential decay rate for the twist (per second)",\n      "parameter_name": "decay_rate",\n      "parameter_type": "double",\n      "default_value": "1.0"\n    },\n    {\n      "display_name": "Reference frame",\n      "description": "Optional reference frame for the output twist. If not set, the camera frame will be used.",\n      "parameter_name": "reference_frame",\n      "parameter_type": "string",\n      "default_value": null,\n      "optional": true\n    }\n  ]\n}\n'})})]}),"\n",(0,a.jsx)(n.p,{children:"Enter the component folder in terminal and run"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"docker build -f aica-package.toml -t object-detection-utils .\n"})}),"\n",(0,a.jsxs)(n.p,{children:["Next, edit the AICA Launcher configuration and enter ",(0,a.jsx)(n.code,{children:"object-detection-utils"})," under ",(0,a.jsx)(n.code,{children:"Custom Packages"}),". After launching,\nyou should see the ",(0,a.jsx)(n.code,{children:"Object detection utils"})," package listed in the ",(0,a.jsx)(n.code,{children:"Add Component"})," menu, as well as a ",(0,a.jsx)(n.code,{children:"BoundingBoxTracker"}),"\ncomponent under that menu."]}),"\n",(0,a.jsx)(n.h3,{id:"application-setup",children:"Application setup"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Add the ",(0,a.jsx)(n.code,{children:"BoundingBoxTracker"})," component to the previously configured application","\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Turn on ",(0,a.jsx)(n.strong,{children:"auto-configure"})," and ",(0,a.jsx)(n.strong,{children:"auto-activate"})]}),"\n",(0,a.jsxs)(n.li,{children:["Set the ",(0,a.jsx)(n.code,{children:"Rate"})," to roughly match your camera's FPS, e.g., to 30"]}),"\n",(0,a.jsxs)(n.li,{children:["Set the ",(0,a.jsx)(n.code,{children:"Control gains"})," to values that are sensible for your robot and your desired responsiveness"]}),"\n",(0,a.jsxs)(n.li,{children:["Set ",(0,a.jsx)(n.code,{children:"Deadband"})," (i.e., a banded region of acceptable error within which no twist is generated) to your liking"]}),"\n",(0,a.jsxs)(n.li,{children:["Set the ",(0,a.jsx)(n.code,{children:"Decay rate"})," (i.e., the rate at which a twist will decay if no object is detected) per your application's\nneeds"]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["Connect the ",(0,a.jsx)(n.code,{children:"Detections"})," output of ",(0,a.jsx)(n.code,{children:"YoloExecutor"})," to the ",(0,a.jsx)(n.code,{children:"Detections"})," input of ",(0,a.jsx)(n.code,{children:"BoundingBoxTracker"})]}),"\n"]}),"\n",(0,a.jsx)(n.h4,{id:"commanding-the-robot-with-the-generated-twist",children:"Commanding the robot with the generated twist"}),"\n",(0,a.jsxs)(n.p,{children:["Add a Hardware Interface node to the application and select the ",(0,a.jsx)(n.code,{children:"Generic six-axis robot arm"})," in the ",(0,a.jsx)(n.code,{children:"URDF"})," selection.\nPress on the ",(0,a.jsx)(n.strong,{children:"+"})," button to add a new controller and select the ",(0,a.jsx)(n.code,{children:"IK Velocity Controller"}),". Make sure to enable the\n",(0,a.jsx)(n.strong,{children:"auto-configure"})," and ",(0,a.jsx)(n.strong,{children:"auto-activate"})," options."]}),"\n",(0,a.jsxs)(n.p,{children:["In your ",(0,a.jsx)(n.code,{children:"CameraStreamer"})," component's settings, set the ",(0,a.jsx)(n.code,{children:"Camera frame"})," parameter to ",(0,a.jsx)(n.code,{children:"tool0"}),". This way, you are naively\nassuming that your camera lens is attached at the center of the robot tool, hence allowing the controller to translate\nthe desired twist."]}),"\n",(0,a.jsxs)(n.p,{children:["Back at your ",(0,a.jsx)(n.code,{children:"BoundingBoxTracker"})," component, connect the ",(0,a.jsx)(n.code,{children:"Twist"})," output of this component to the ",(0,a.jsx)(n.code,{children:"Command"})," input of the\n",(0,a.jsx)(n.code,{children:"IK Velocity Controller"}),"."]}),"\n",(0,a.jsx)(n.admonition,{type:"tip",children:(0,a.jsxs)(n.p,{children:["If you are using one of the other robot models that AICA offers, make sure to change the ",(0,a.jsx)(n.code,{children:"Camera frame"})," parameter to\nyour robot's end-effector frame, or to record a frame in world coordinates from the ",(0,a.jsx)(n.code,{children:"3D Viz"})," menu."]})}),"\n",(0,a.jsx)(n.p,{children:"You are now all set to run this application. For reference and a quick visual validation, the final graph should look\nlike the following picture:"}),"\n",(0,a.jsx)("div",{class:"text--center",children:(0,a.jsx)("img",{src:l,alt:"Bounding box tracker application overview"})}),"\n",(0,a.jsxs)(n.p,{children:["If you copied the code from this example, the ",(0,a.jsx)(n.code,{children:"YoloExecutor"})," will be set to track a pair of ",(0,a.jsx)(n.strong,{children:"scissors"})," across the\nframe. Pick up a pair, play the application, and see how the robot adapts to your movements. Remember, in a real-world\nscenario the camera would be attached to the robot and motion would stop as soon as the object was centered. Here,\nhowever, the camera is fixed and motionless, so you have to position the object at the middle of your camera frame to\nprevent the robot from moving in the 2D plane."]}),"\n",(0,a.jsxs)(n.p,{children:["Once you have tested this application, go ahead and pick another object that is included in the\n",(0,a.jsx)(n.a,{href:"https://github.com/ultralytics/ultralytics/blob/main/ultralytics/cfg/datasets/coco.yaml",children:"coco"})," dataset and try again.\nSome objects are easier to recognize than others, so you may have to adapt the ",(0,a.jsx)(n.code,{children:"YoloExecutor"}),"'s parameters or even opt\nfor a larger YOLO model."]}),"\n",(0,a.jsx)(n.admonition,{type:"tip",children:(0,a.jsx)(n.p,{children:"If you are planning to use this demo with a physical robot, additional logic and safety considerations may have to be\nmade."})}),"\n",(0,a.jsx)(n.h3,{id:"application-code",children:"Application code"}),"\n",(0,a.jsxs)(t,{children:[(0,a.jsx)("summary",{children:"YAML application"}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:"schema: 2-0-6\ndependencies:\n  core: v5.0.0\non_start:\n  load:\n    - hardware: hardware\n    - component: camera_streamer\ncomponents:\n  yolo_executor:\n    component: advanced_perception::object_detection::YoloExecutor\n    display_name: YOLO Executor\n    events:\n      transitions:\n        on_load:\n          lifecycle:\n            component: yolo_executor\n            transition: configure\n        on_configure:\n          lifecycle:\n            component: yolo_executor\n            transition: activate\n        on_activate:\n          load:\n            component: bounding_box_tracker\n    parameters:\n      rate:\n        value: 30\n        type: double\n      model_file:\n        value: /yolo-example-data/yolo12n.onnx\n        type: string\n      classes_file:\n        value: /yolo-example-data/coco.yaml\n        type: string\n      object_class:\n        value:\n          - scissors\n        type: string_array\n      num_threads:\n        value: 4\n        type: int\n    inputs:\n      image: /camera_streamer/image\n    outputs:\n      detections: /yolo_executor/detections\n  camera_streamer:\n    component: core_vision_components::image_streaming::CameraStreamer\n    display_name: Camera Streamer\n    events:\n      transitions:\n        on_load:\n          lifecycle:\n            component: camera_streamer\n            transition: configure\n        on_configure:\n          lifecycle:\n            component: camera_streamer\n            transition: activate\n        on_activate:\n          load:\n            component: yolo_executor\n    parameters:\n      camera_frame:\n        value: tool0\n        type: string\n    outputs:\n      image: /camera_streamer/image\n  bounding_box_tracker:\n    component: object_detection_utils::BoundingBoxTracker\n    display_name: Bounding box tracker\n    events:\n      transitions:\n        on_load:\n          lifecycle:\n            component: bounding_box_tracker\n            transition: configure\n        on_configure:\n          lifecycle:\n            component: bounding_box_tracker\n            transition: activate\n    parameters:\n      rate:\n        value: 100\n        type: double\n      gains:\n        value:\n          - 0.001\n          - 0.001\n        type: double_array\n    inputs:\n      detections: /yolo_executor/detections\n    outputs:\n      twist: /yolo_to_marker/twist\nhardware:\n  hardware:\n    display_name: Hardware Interface\n    urdf: Generic six-axis robot arm\n    rate: 100\n    events:\n      transitions:\n        on_load:\n          load:\n            - controller: robot_state_broadcaster\n              hardware: hardware\n            - controller: ik_velocity_controller\n              hardware: hardware\n    controllers:\n      robot_state_broadcaster:\n        plugin: aica_core_controllers/RobotStateBroadcaster\n        events:\n          transitions:\n            on_load:\n              switch_controllers:\n                hardware: hardware\n                activate: robot_state_broadcaster\n      ik_velocity_controller:\n        plugin: aica_core_controllers/velocity/IKVelocityController\n        inputs:\n          command: /yolo_to_marker/twist\n        events:\n          transitions:\n            on_load:\n              switch_controllers:\n                hardware: hardware\n                activate: ik_velocity_controller\ngraph:\n  positions:\n    on_start:\n      x: -20\n      y: -360\n    stop:\n      x: -20\n      y: -260\n    components:\n      yolo_executor:\n        x: 740\n        y: -180\n      camera_streamer:\n        x: 200\n        y: -300\n      bounding_box_tracker:\n        x: 1400\n        y: 180\n    hardware:\n      hardware:\n        x: 1940\n        y: -360\n  edges:\n    yolo_to_marker_marker_pose_signal_point_attractor_attractor:\n      path:\n        - x: 1360\n          y: 520\n        - x: 1360\n          y: 680\n    yolo_executor_detections_yolo_to_marker_json_input:\n      path:\n        - x: 1160\n          y: 120\n        - x: 1160\n          y: 220\n        - x: 860\n          y: 220\n        - x: 860\n          y: 520\n    yolo_to_marker_twist_hardware_hardware_ik_velocity_controller_command:\n      path:\n        - x: 1820\n          y: 380\n        - x: 1820\n          y: 420\n    yolo_executor_detections_yolo_to_marker_detections:\n      path:\n        - x: 1200\n          y: 120\n        - x: 1200\n          y: 380\n    yolo_executor_on_activate_bounding_box_tracker_bounding_box_tracker:\n      path:\n        - x: 1300\n          y: 0\n        - x: 1300\n          y: 240\n    yolo_executor_detections_bounding_box_tracker_detections:\n      path:\n        - x: 1220\n          y: 120\n        - x: 1220\n          y: 400\n    on_start_on_start_camera_streamer_camera_streamer:\n      path:\n        - x: 140\n          y: -300\n        - x: 140\n          y: -240\n    camera_streamer_image_yolo_executor_image:\n      path:\n        - x: 640\n          y: -40\n        - x: 640\n          y: 120\n"})})]})]})}function _(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(b,{...e})}):b(e)}}}]);